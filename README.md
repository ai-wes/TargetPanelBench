# TargetPanelBench

TargetPanelBench is a public benchmarking framework for evaluating computational drug‐target prioritisation and panel design algorithms.  It provides curated example data, baseline algorithms, evaluation metrics and a reproducible workflow to compare the performance of different methods on realistic tasks.

The overarching goal of this repository is to standardise how the field measures the effectiveness of target narrowing strategies and to demonstrate the benefits of Archipelago's Adaptive Ensemble Algorithm (AEA) through transparent, reproducible experiments.  While the AEA implementation itself remains proprietary, we include precomputed results from this method so that you can directly compare your own approaches against a strong baseline.

## Repository structure

```
TargetPanelBench/
├── README.md                 # This file
├── download_data.py          # Script to generate or download the benchmark dataset
├── baselines/                # Baseline ranking algorithms
│   ├── __init__.py
│   ├── simple_score_rank.py  # Naïve sum-of-features ranking
│   └── cma_es.py             # Evolutionary optimisation of feature weights
├── evaluation.py             # Shared evaluation metrics
├── panel_design.py           # Greedy panel selection and diversity scoring
├── data/                     # Input datasets (generated by download_data.py)
│   ├── targets.csv           # Candidate targets with features and ground truth labels
│   └── ppi_edges.csv         # Protein–protein interaction network (edge list)
├── results/                  # Example results from baseline methods and AEA
│   ├── simple_results.json
│   ├── cma_es_results.json
│   └── aea_results.json      # Precomputed results from the proprietary method
└── notebooks/
    └── Run_Benchmark.ipynb   # Walk‑through Jupyter notebook
```

## Installation

This project relies on Python 3.9+ and a few common scientific libraries.  You can install the dependencies in a new virtual environment with:

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

The provided `requirements.txt` file contains only widely used, open‑source packages such as `numpy`, `pandas`, `networkx` and `matplotlib`.

## Generating the dataset

Run the following command from the root of the repository to generate the benchmark data.  By default this script synthesises a reproducible dataset with 100 candidate genes and randomly generated features.  You can extend or replace the data generation logic with API calls to real data sources (e.g. OpenTargets, GTEx, STRING) if you wish.

```bash
python download_data.py
```

This will create the following files in the `data/` folder:

* `targets.csv` – A CSV file containing one row per candidate gene with the following columns:
  * `gene_id`: Stable gene identifier
  * `gene_name`: Approved gene symbol
  * `ground_truth`: `1` if the gene is part of the positive set (the “known good” targets) and `0` otherwise
  * `genetic_association`: A synthetic numeric feature (0–1) representing genetic evidence strength
  * `gene_expression`: A synthetic numeric feature (0–1) representing tissue‑specific expression
  * `druggability`: A synthetic numeric feature (0–1) representing chemical tractability
  * `literature_velocity`: A synthetic numeric feature (0–1) representing publication trend

* `ppi_edges.csv` – A CSV file containing an edge list for the protein–protein interaction network.  Each row describes an undirected interaction between two genes (`gene1`, `gene2`) with an optional `weight` column equal to 1.

## Running the benchmark

After generating the data, you can run each baseline method individually.  For example:

```bash
# Naïve sum of features
python baselines/simple_score_rank.py --data-dir data --results-path results/simple_results.json

# Evolutionary optimisation of weights
python baselines/cma_es.py --data-dir data --results-path results/cma_es_results.json --iterations 100
```

Each baseline script outputs a JSON file containing the ranking, selected panel and evaluation metrics.  These results are also printed to the console for convenience.

## Evaluating your own method

If you have developed your own ranking algorithm, you can use the shared `evaluation.py` and `panel_design.py` modules to compute standard metrics on your output.  A basic usage example is shown below:

```python
from evaluation import compute_ranking_metrics
from panel_design import select_diverse_panel
import pandas as pd
import json
import networkx as nx

# Load the candidate data and PPI network
targets = pd.read_csv('data/targets.csv')
edges = pd.read_csv('data/ppi_edges.csv')
G = nx.from_pandas_edgelist(edges, 'gene1', 'gene2', create_using=nx.Graph())

# Suppose `my_ranking` is a list of gene_ids ordered from best to worst
metrics = compute_ranking_metrics(targets, my_ranking, k=20)
panel = select_diverse_panel(my_ranking, G, panel_size=12)

# Calculate panel recall and diversity
from evaluation import panel_recall, panel_diversity_score

recall = panel_recall(panel, targets)
diversity = panel_diversity_score(panel, G)

print(metrics)
print({'panel_recall': recall, 'panel_diversity': diversity})
```

### Unified CLI

You can orchestrate baselines and public tool adapters via a single CLI:

```bash
# Generate synthetic data
python download_data.py --num-genes 200 --num-positives 40

# Run baselines on synthetic data
python benchmark.py run-baselines --data-dir data --out results/baselines.json

# Run OpenTargets adapter for Type 2 Diabetes (requires internet)
python benchmark.py run-opentargets --efo-id EFO_0003767 --out results/ot_t2d.json

# Summarise all JSON results into a CSV
python benchmark.py summarise --results-dir results --out results/summary.csv

# Run OpenTargets on local Parquet (DOID_* disease IDs)
python benchmark.py run-opentargets-local \
  --ot-dir /mnt/c/Users/wes/Desktop/open_targets_data \
  --disease-id DOID_0050890 \
  --out results/ot_local_DOID_0050890.json
```

See `docs/methods.md` and `docs/benchmark_protocol.md` for details.

## Jupyter notebook

The `notebooks/Run_Benchmark.ipynb` notebook walks through the full benchmark pipeline.  It loads the data, runs the baseline methods, visualises the results and compares them to the precomputed AEA outputs.  This notebook is intended as both an educational resource and a demonstration of the framework’s reproducibility.

## Precomputed results

For convenience, the repository contains precomputed results for both the baseline methods and the proprietary Adaptive Ensemble Algorithm (AEA).  These JSON files in the `results/` folder include the rankings, selected panels and metrics.  You can compare your own method against these numbers or reproduce the results by running the provided scripts.

| Method                 | Precision@20 | MRR   | nDCG@20 | Panel Recall | Panel Diversity |
|------------------------|-------------:|------:|--------:|-------------:|----------------:|
| Simple Score & Rank    | 0.35        | 0.56  | 0.61    | 0.50         | 3.8             |
| CMA‑ES Weighted Sum    | 0.40        | 0.60  | 0.65    | 0.60         | 4.1             |
| **Archipelago AEA**    | **0.55**    | **0.72** | **0.77** | **0.70**     | **4.8**         |

*Table 1 – Example benchmark results on the synthetic dataset.  The Adaptive Ensemble Algorithm (AEA) consistently outperforms naïve and evolutionary baselines across both ranking and panel design metrics.*

## Contributing

Contributions are welcome!  Feel free to open issues or pull requests to add new baseline methods, extend the dataset generator, or improve the evaluation metrics.  Please ensure that any new code is well documented and includes unit tests where appropriate.

---
This project is released under the MIT licence.  See `LICENSE` for details.